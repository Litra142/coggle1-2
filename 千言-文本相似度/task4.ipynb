{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c32471a",
   "metadata": {},
   "source": [
    "任务4：使用中文词向量完成mean/max/sif句子编码\n",
    "\n",
    "- 步骤1 ：单词通过word2vec编码为100维向量，则句子编码为N∗100N∗100的矩阵，N为句子单词个数。\n",
    "- 步骤2 ：将N*100的矩阵进行max-pooling编码，转为100维度。\n",
    "- 步骤3 ：将N*100的矩阵进行mean-pooling编码，转为100维度。\n",
    "- 步骤4 ：将N*100的矩阵与单词的IDF进行矩阵相乘，即按照单词的词频进行加权，进行tfidf-pooling编码，转为100维度。\n",
    "- 步骤5 ：学习SIF编码的原理，进行sif编码，转为100维度。\n",
    "> https://github.com/PrincetonML/SIF/blob/master/src/SIF_embedding.py#L30\n",
    "\n",
    "> https://openreview.net/pdf?id=SyK00v5xx\n",
    "- 步骤6（可选） ：通过上述步骤2-步骤5的编码，计算相似句子的相似度 vs 不相似句子的相似度， 绘制得到分布图，哪一种编码最优？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "59072ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#隐藏警告\n",
    "import warnings\n",
    "import jieba\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5e9b4050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>sentences1</th>\n",
       "      <th>sentences2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>用微信都6年，微信没有微粒贷功能</td>\n",
       "      <td>4。号码来微粒贷</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>微信消费算吗</td>\n",
       "      <td>还有多少钱没还</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>交易密码忘记了找回密码绑定的手机卡也掉了</td>\n",
       "      <td>怎么最近安全老是要改密码呢好麻烦</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>你好我昨天晚上申请的没有打电话给我今天之内一定会打吗？</td>\n",
       "      <td>什么时候可以到账</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>“微粒贷开通\"</td>\n",
       "      <td>你好，我的微粒贷怎么没有开通呢</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                   sentences1        sentences2 label\n",
       "0      1             用微信都6年，微信没有微粒贷功能          4。号码来微粒贷     0\n",
       "1      2                       微信消费算吗           还有多少钱没还     0\n",
       "2      3         交易密码忘记了找回密码绑定的手机卡也掉了  怎么最近安全老是要改密码呢好麻烦     0\n",
       "3      4  你好我昨天晚上申请的没有打电话给我今天之内一定会打吗？          什么时候可以到账     0\n",
       "4      5                      “微粒贷开通\"   你好，我的微粒贷怎么没有开通呢     0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_tsv(input_file,columns):\n",
    "    with open(input_file,\"r\",encoding=\"utf-8\") as file:\n",
    "        lines = []\n",
    "        count = 1\n",
    "        for line in file:\n",
    "            if len(line.strip().split(\"\\t\")) != 1:\n",
    "                lines.append([count]+line.strip().split(\"\\t\"))\n",
    "                count += 1\n",
    "        df = pd.DataFrame(lines)\n",
    "        df.columns = columns\n",
    "    return df\n",
    "bq_train = read_tsv('data/bq_corpus/train.tsv',['index','sentences1','sentences2','label'])\n",
    "bq_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5809d0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#大写字母转为小写字母\n",
    "def upper2lower(sentence):\n",
    "    new_sentence=sentence.lower()\n",
    "    return new_sentence\n",
    "bq_train['chinese_sentences1'] = bq_train['sentences1'].apply(upper2lower)\n",
    "bq_train['chinese_sentences2'] = bq_train['sentences2'].apply(upper2lower)\n",
    "\n",
    "#去除文本中的表情字符（只保留中英文和数字）\n",
    "import re\n",
    "def clear_character(sentence):\n",
    "    pattern1= '\\[.*?\\]'     \n",
    "    pattern2 = re.compile('[^\\u4e00-\\u9fa5^a-z^A-Z^0-9]')   \n",
    "    line1=re.sub(pattern1,'',sentence)\n",
    "    line2=re.sub(pattern2,'',line1)   \n",
    "    new_sentence=''.join(line2.split()) #去除空白\n",
    "    return new_sentence\n",
    "bq_train['chinese_sentences1'] = bq_train['chinese_sentences1'].apply(clear_character)\n",
    "bq_train['chinese_sentences2'] = bq_train['chinese_sentences2'].apply(clear_character)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "572fed06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Litra\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.466 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "def segment_sen(sen):\n",
    "    sen_list = []\n",
    "    try:\n",
    "        sen_list = jieba.lcut(sen)\n",
    "    except:\n",
    "        pass\n",
    "    return sen_list\n",
    "sen1_list = [segment_sen(i) for i in bq_train['chinese_sentences1']]\n",
    "sen2_list = [segment_sen(i) for i in bq_train['chinese_sentences2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc4f6e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#先读取w2v_model\n",
    "from gensim.models import Word2Vec\n",
    "model1 = Word2Vec.load('word2vec_1.model')\n",
    "model2 = Word2Vec.load('word2vec_2.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7f9193e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.text import TextCollection\n",
    "\n",
    "#构建sentences1和sentences2的语料库corpus1和corpus2\n",
    "corpus1 = TextCollection(sen1_list)  \n",
    "corpus2 = TextCollection(sen2_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a810cb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#得到sif权重\n",
    "def get_SIF_weight(a, sentences, corpus):\n",
    "    SIF_weight = {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            SIF_weight[word] =  a / a + corpus.tf_idf(word, sentence)\n",
    "    return SIF_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "35887059",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0.001\n",
    "SIF_weight1 = get_SIF_weight(a, sen1_list, corpus1)\n",
    "SIF_weight2 = get_SIF_weight(a, sen2_list, corpus2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dfd656df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#获得基于SIF改进后的句子向量-->输入的是单个句子，返回的\n",
    "import math\n",
    "def build_sentences_vector_sif_weight(sentences,size,w2v_model,sif_weight):\n",
    "    all_sentences_metrix = np.zeros((1, size))\n",
    "    sen_vec=np.zeros(size).reshape((1,size))\n",
    "    for index, sentence in enumerate(sentences):\n",
    "        count=0\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                if word in sif_weight.keys():\n",
    "                    sen_vec+=(np.dot(w2v_model.wv[word],math.exp(sif_weight[word]*0.001))).reshape((1,size))\n",
    "                    count+=1\n",
    "                else:\n",
    "                    sen_vec+=w2v_model[word].reshape((1,size))\n",
    "                    count+=1\n",
    "            except KeyError:\n",
    "                continue\n",
    "        if count!=0:\n",
    "            sen_vec/=count\n",
    "        if index == 0:\n",
    "            all_sentences_metrix = sen_vec\n",
    "        else:\n",
    "            all_sentences_metrix = np.vstack((all_sentences_metrix, sen_vec))\n",
    "    return all_sentences_metrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79240f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrix_sen1 = build_sentences_vector_sif_weight(sen1_list, 100, model1, SIF_weight1)\n",
    "metrix_sen2 = build_sentences_vector_sif_weight(sen2_list, 100, model2, SIF_weight2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8691fe48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "def compute_pc(X,npc=1):\n",
    "    \"\"\"\n",
    "    Compute the principal components. DO NOT MAKE THE DATA ZERO MEAN!\n",
    "    :param X: X[i,:] is a data point\n",
    "    :param npc: number of principal components to remove\n",
    "    :return: component_[i,:] is the i-th pc\n",
    "    \"\"\"\n",
    "    svd = TruncatedSVD(n_components=npc, n_iter=7, random_state=0)\n",
    "    svd.fit(X)\n",
    "    return svd.components_\n",
    "\n",
    "def remove_pc(X, npc=1):\n",
    "    \"\"\"\n",
    "    Remove the projection on the principal components\n",
    "    :param X: X[i,:] is a data point\n",
    "    :param npc: number of principal components to remove\n",
    "    :return: XX[i, :] is the data point after removing its projection\n",
    "    \"\"\"\n",
    "    pc = compute_pc(X, npc)\n",
    "    if npc==1:\n",
    "        XX = X - X.dot(pc.transpose()) * pc\n",
    "    else:\n",
    "        XX = X - X.dot(pc.transpose()).dot(pc)\n",
    "    return XX\n",
    "\n",
    "\n",
    "def SIF_embedding(sentences, size, w2v_model, sif_weight, npc):\n",
    "    \"\"\"\n",
    "    Compute the scores between pairs of sentences using weighted average + removing the projection on the first principal component\n",
    "    :param We: We[i,:] is the vector for word i\n",
    "    :param x: x[i, :] are the indices of the words in the i-th sentence\n",
    "    :param w: w[i, :] are the weights for the words in the i-th sentence\n",
    "    :param params.rmpc: if >0, remove the projections of the sentence embeddings to their first principal component\n",
    "    :return: emb, emb[i, :] is the embedding for sentence i\n",
    "    \"\"\"\n",
    "    emb = build_sentences_vector_sif_weight(sentences,size,w2v_model,sif_weight)\n",
    "    if  npc > 0:\n",
    "        emb = remove_pc(emb, npc)\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5ea1696e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sif_embedding_1= SIF_embedding(sen1_list, 100, model1, SIF_weight1, 1)\n",
    "sif_embedding_2= SIF_embedding(sen2_list, 100, model2, SIF_weight2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "33bd8a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.1631529 ,  0.68570732, -0.56345899, ...,  0.05019303,\n",
       "         0.08823364,  0.73780842],\n",
       "       [ 0.1631529 ,  0.68570732, -0.56345899, ...,  0.05019303,\n",
       "         0.08823364,  0.73780842],\n",
       "       [-0.45038705,  0.14196152, -0.29118772, ..., -0.94526421,\n",
       "         0.30650289,  0.00765647],\n",
       "       ...,\n",
       "       [-0.17717167, -0.1151685 ,  0.64575971, ...,  0.43572011,\n",
       "        -0.2061713 , -0.20033421],\n",
       "       [-0.24399607, -0.0214033 ,  0.93929948, ...,  0.97110859,\n",
       "        -0.61965028, -0.61955608],\n",
       "       [ 0.11829696, -0.06113644, -0.26674104, ..., -0.30710764,\n",
       "         0.16443198, -0.07984093]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sif_embedding_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ae480f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('sif_embedding1.txt', sif_embedding_1)\n",
    "np.savetxt('sif_embedding2.txt', sif_embedding_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4eaa1d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
